{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41be77f-ea96-40f0-97a7-fd53ee73c5f7",
   "metadata": {},
   "source": [
    "# Hugging Face for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5b95f-d490-4a28-8a56-facaa4f48a0f",
   "metadata": {},
   "source": [
    "Inference on an HPC system is fine for testing, however it does not make sense to host models on an HPC system, since you need to send requests off as SLURM jobs.\n",
    "Still, here on the JupyterHub we can have a look at how inference works.\n",
    "Ideally, we would have a GPU available, but we can also make do with CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91620ce-23c3-41fc-aae7-aaa524780fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.3\"  # You can use any pre-trained model here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Set the padding token to be the EOS token (or specify a custom one if needed)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1911213-6282-40a1-b2a8-b90a51a90cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a time\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1d4fe-af8b-48f7-95a2-6ff3b08fab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3f518-0ecd-44f7-b812-671a555e60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_texts = [\"Once upon a time\", \"The quick brown fox\"]\n",
    "inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input: {batch_texts[i]}\")\n",
    "    print(f\"Generated: {tokenizer.decode(output, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e7fbd-04f3-46dc-8970-badd7debc980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed327c6-11a3-478b-89ca-933b503e0764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
