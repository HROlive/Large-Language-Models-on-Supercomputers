{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b1f163-108f-4587-8b0d-0246158ee528",
   "metadata": {},
   "source": [
    "# FSDP example with Mistral-7B and Guanaco dataset\n",
    "In this example a network is trained on multiple GPUs with the help of FSDP (Fully Sharded Data Parallel). This approach allows to train networks that are too large to fit into the memory of a single GPU.\n",
    "\n",
    "If we want to use multiple GPUs, we need to write the code to a file and submit the job to the SLURM scheduler, because JupyterHub at VSC is configured to have access to only one GPU at maximum. This example uses two GPUs on one node, but could be extended to use multiple nodes simply by adjusting the number of nodes in the line\n",
    "```\n",
    "#SBATCH --nodes=1\n",
    "```\n",
    "in the SLURM script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ee84-e29b-4c05-b124-50e735033760",
   "metadata": {},
   "source": [
    "#### First, we write the python code to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565c4533-5104-4a7c-a688-8b6acb72e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mistral7b_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mistral7b_train.py\n",
    "import torch\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import pynvml\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used/1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "data = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "\n",
    "ps = PartialState()\n",
    "num_processes = ps.num_processes\n",
    "process_index = ps.process_index\n",
    "local_process_index = ps.local_process_index\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation='sdpa',  # 'eager', 'sdpa', or \"flash_attention_2\"\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1  # disable tensor parallelism\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    target_modules='all-linear',\n",
    ")\n",
    "\n",
    "project_name = 'mistral7b-guanaco'\n",
    "run_name = '1'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=f'{project_name}-{run_name}',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 12 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 8 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    optim='adamw_torch',  # 'paged_adamw_32bit' can save GPU memory\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=50,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    max_steps=10,\n",
    "    fp16=True,  # mixed precision training\n",
    "    report_to='none',  # disable wandb\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "if process_index == 0:  # Only print in first process.\n",
    "    if hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "# Print statistics in first process only:\n",
    "if process_index == 0:\n",
    "    print(f\"Run time: {result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"{num_processes} GPUs used.\")\n",
    "    print(f\"Training speed: {result.metrics['train_samples_per_second']:.1f} samples/s (={result.metrics['train_samples_per_second'] / num_processes:.1f} samples/s/GPU)\")\n",
    "\n",
    "# Print memory usage once per node:\n",
    "if local_process_index == 0:\n",
    "    print_gpu_utilization()\n",
    "\n",
    "# Save model in first process only:\n",
    "if process_index == 0:\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e5aa1-7732-481f-a8db-451360ba6d74",
   "metadata": {},
   "source": [
    "#### Next, we write a file with the configuration for FSDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e80b63-c5f8-4166-b0dc-4d48c9b4c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fsdp_config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile fsdp_config.yml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: FSDP\n",
    "downcast_bf16: 'no'\n",
    "fsdp_config:\n",
    "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
    "  fsdp_backward_prefetch: BACKWARD_PRE\n",
    "  fsdp_cpu_ram_efficient_loading: true\n",
    "  fsdp_forward_prefetch: false\n",
    "  fsdp_offload_params: false\n",
    "  fsdp_sharding_strategy: FULL_SHARD\n",
    "  fsdp_state_dict_type: SHARDED_STATE_DICT\n",
    "  fsdp_sync_module_states: true\n",
    "  fsdp_use_orig_params: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: c10d\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1769457-c82f-4954-89a5-7b3b47ed72cc",
   "metadata": {},
   "source": [
    "#### Finally, we write the SLURM script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73291081-25e7-4578-a944-716d4e29d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vsc5a100_fsdp.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vsc5a100_fsdp.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=zen3_0512_a100x2\n",
    "# #SBATCH --qos=zen3_0512_a100x2\n",
    "#SBATCH --qos=admin\n",
    "\n",
    "## Specify resources:\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:2  # up to 2 on VSC5/A100\n",
    "#SBATCH --ntasks-per-node=1\n",
    "## No need to specify RAM on VSC5, as it will be automatically\n",
    "## allocated depending on the number of GPUs requested.\n",
    "\n",
    "#SBATCH --time=0:30:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load miniconda3\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=24998\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "\n",
    "# Print statistics:\n",
    "echo \"Using $((SLURM_NNODES * SLURM_GPUS_ON_NODE)) GPUs on $SLURM_NNODES nodes.\"\n",
    "\n",
    "# Run AI scripts:\n",
    "srun bash -c \"conda run -n finetuning --no-capture-output accelerate launch \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    --num_processes $((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n",
    "    --num_cpu_threads_per_process 8 \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --config_file \\\"fsdp_config.yml\\\" \\\n",
    "    mistral7b_train.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f174-8231-4e1e-ae39-bff66ffccddc",
   "metadata": {},
   "source": [
    "#### We can now execute the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cc6fe-ec18-4856-b99a-e1e2f4f5ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch run_vsc5a100_fsdp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886c3e1-da04-49b9-bf9d-806083239ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1677b-5e3b-4a54-85e9-90d04ddd8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -c +0 slurm-3990535.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5de1c-fa75-4dc8-a5b3-893b8694261c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52fe7c-9bfe-45d0-84ed-9287c9c84f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm fsdp_config.yml mistral7b_train.py run_vsc5a100_fsdp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf21d02-c9f3-4413-b950-48da652f3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d8108-1e84-45c2-8560-af8958deb599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f249b8-c1d0-4f98-881f-5174351dc837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Output of the same script executed on Leonardo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047abe8d-5e17-448c-aa87-b8c63c7b7103",
   "metadata": {},
   "source": [
    "```\n",
    "Unloading profile/base\n",
    "  ERROR: Module evaluation aborted\n",
    "+ date\n",
    "Wed Sep 25 19:27:04 CEST 2024\n",
    "+ hostname\n",
    "lrdn3361.leonardo.local\n",
    "+ nvidia-smi\n",
    "Wed Sep 25 19:27:04 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM-64GB            On | 00000000:8F:00.0 Off |                    0 |\n",
    "| N/A   43C    P0               61W / 455W|      0MiB / 65536MiB |      0%      Default |\n",
    "|                                         |                      |             Disabled |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-SXM-64GB            On | 00000000:C8:00.0 Off |                    0 |\n",
    "| N/A   42C    P0               61W / 458W|      0MiB / 65536MiB |      0%      Default |\n",
    "|                                         |                      |             Disabled |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|  No running processes found                                                           |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "+ export MASTER_PORT=24998\n",
    "+ MASTER_PORT=24998\n",
    "++ scontrol show hostnames lrdn3361\n",
    "++ head -n 1\n",
    "+ export MASTER_ADDR=lrdn3361\n",
    "+ MASTER_ADDR=lrdn3361\n",
    "+ echo 'Using 2 GPUs on 1 nodes.'\n",
    "Using 2 GPUs on 1 nodes.\n",
    "+ srun bash -c 'conda run -n finetuning --no-capture-output accelerate launch     --num_machines 1     --num_processes 2     --num_cpu_threads_per_process 8     --main_process_ip lrdn3361     --main_process_port 24998     --machine_rank $SLURM_PROCID     --config_file \"fsdp_config.yml\"     mistral7b_train.py'\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Loading checkpoint shards: 100%|__________| 3/3 [02:56<00:00, 58.79s/it]\n",
    "Loading checkpoint shards: 100%|__________| 3/3 [02:55<00:00, 58.48s/it]\n",
    "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
    "max_steps is given, it will override any value given in num_train_epochs\n",
    "max_steps is given, it will override any value given in num_train_epochs\n",
    "trainable params: 41,943,040 || all params: 7,289,966,592 || trainable%: 0.5754\n",
    "{'train_runtime': 30.6162, 'train_samples_per_second': 5.226, 'train_steps_per_second': 0.327, 'train_loss': 1.3052967071533204, 'epoch': 0.02}\n",
    "100%|__________| 10/10 [00:30<00:00,  3.06s/it]\n",
    "Run time: 30.62 seconds\n",
    "2 GPUs used.\n",
    "Training speed: 5.2 samples/s (=2.6 samples/s/GPU)\n",
    "Memory occupied on GPUs: 18.1 + 14.4 GB.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b7d0b-924a-46f1-a23f-42ee5964dfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
