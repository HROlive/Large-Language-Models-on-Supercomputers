{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b1f163-108f-4587-8b0d-0246158ee528",
   "metadata": {},
   "source": [
    "# DDP example with Mistral-7B and medmcqa dataset\n",
    "In this example a network is trained on multiple GPUs with the help of DDP (Distributed Data Parallel). This approach allows to train networks that fit into the memory of a single GPU on multiple GPUs in parallel in order to speed up the training.\n",
    "\n",
    "If we want to use multiple GPUs, we need to write the code to a file and submit the job to the SLURM scheduler, because JupyterHub at VSC is configured to have access to only one GPU at maximum.\n",
    "\n",
    "This example uses 2 nodes with 2 GPUs each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ee84-e29b-4c05-b124-50e735033760",
   "metadata": {},
   "source": [
    "#### First, we write the python code to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c4533-5104-4a7c-a688-8b6acb72e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mistral7b_train_ddp.py\n",
    "import torch\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import random\n",
    "from textwrap import dedent  # Remove leading whitespace from multiline strings\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    memory_used = []\n",
    "    for device_index in range(device_count):\n",
    "        device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n",
    "        device_info = pynvml.nvmlDeviceGetMemoryInfo(device_handle)\n",
    "        memory_used.append(device_info.used/1024**3)\n",
    "    print('Memory occupied on GPUs: ' + ' + '.join([f'{mem:.1f}' for mem in memory_used]) + ' GB.')\n",
    "\n",
    "def medmcqa_get_answer(entry):\n",
    "    # entry['cop'] is an integer in the range 0..3 that\n",
    "    # denotes the correct option (a, b, c or d).\n",
    "    options = {0:'opa', 1:'opb', 2:'opc', 3:'opd'}\n",
    "    correct_option = options[entry['cop']]\n",
    "    answer = entry[correct_option]\n",
    "    return answer\n",
    "\n",
    "def medmcqa_add_prompt(entry, tokenizer, include_answer, shuffle_options=False):\n",
    "    options = [\n",
    "        entry[\"opa\"],\n",
    "        entry[\"opb\"],\n",
    "        entry[\"opc\"],\n",
    "        entry[\"opd\"]\n",
    "    ]\n",
    "    if shuffle_options:\n",
    "        random.shuffle(options)\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': dedent(f'''\\\n",
    "            You are a medical student taking a multiple-choice exam. Four options are provided for each question. Only one of these options is the correct answer.\n",
    "            Question: {entry[\"question\"]}\n",
    "            Options:\n",
    "            1. {options[0]}\n",
    "            2. {options[1]}\n",
    "            3. {options[2]}\n",
    "            4. {options[3]}\n",
    "            Solve this multiple-choice exam question and provide the correct answer.''')\n",
    "        }\n",
    "    ]\n",
    "    if include_answer:\n",
    "        answer = medmcqa_get_answer(entry)\n",
    "        messages.append(\n",
    "            {'role': 'assistant', 'content': f'Answer: {answer}'}\n",
    "        )\n",
    "    entry['text'] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return entry\n",
    "\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "max_seq_length = 1024\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "data = load_dataset('medmcqa', split='train')\n",
    "data = data.map(lambda entry:medmcqa_add_prompt(entry, tokenizer, include_answer=True), load_from_cache_file=True)\n",
    "\n",
    "ps = PartialState()\n",
    "num_processes = ps.num_processes\n",
    "process_index = ps.process_index\n",
    "local_process_index = ps.local_process_index\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation='sdpa',  # 'eager', 'sdpa', or \"flash_attention_2\"\n",
    "    device_map={'':local_process_index}\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1  # disable tensor parallelism\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    target_modules='all-linear',\n",
    ")\n",
    "\n",
    "project_name = 'mistral7b-medmcqa'\n",
    "run_name = '1'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    # When using newer versions of `trl`, use SFTConfig(...) instead of TrainingArguments(...).\n",
    "    output_dir=f'{project_name}-{run_name}',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 12 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 8 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    ddp_find_unused_parameters=False,  # Set to False when using gradient checkpointing to suppress warning message.\n",
    "    log_level_replica='error',  # Disable warnings in all but the first process.\n",
    "    optim='adamw_torch',  # 'paged_adamw_32bit' can save GPU memory\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=50,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=20,\n",
    "    fp16=True,  # mixed precision training: faster, but uses more memory\n",
    "    # hub_private_repo=True,\n",
    "    report_to='none',  # disable wandb\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    # When using newer versions of `trl`, the argument `training_arguments` should be given as\n",
    "    # an instance of SFTConfig(...) instead of TrainingArguments(...) and the following\n",
    "    # parameters should be specified there instead of here:\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "if process_index == 0:  # Only print in first process.\n",
    "    if hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "# Print statistics in first process only:\n",
    "if process_index == 0:\n",
    "    print(f\"Run time: {result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"{num_processes} GPUs used.\")\n",
    "    print(f\"Training speed: {result.metrics['train_samples_per_second']:.1f} samples/s (={result.metrics['train_samples_per_second'] / num_processes:.1f} samples/s/GPU)\")\n",
    "\n",
    "# Print memory usage once per node:\n",
    "if local_process_index == 0:\n",
    "    print_gpu_utilization()\n",
    "\n",
    "# Save model in first process only:\n",
    "if process_index == 0:\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1769457-c82f-4954-89a5-7b3b47ed72cc",
   "metadata": {},
   "source": [
    "#### Next, we write the SLURM script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73291081-25e7-4578-a944-716d4e29d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_vsc5a100_ddp.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=zen3_0512_a100x2\n",
    "# #SBATCH --qos=zen3_0512_a100x2\n",
    "#SBATCH --qos=admin\n",
    "#SBATCH --gres=gpu:2  # Number of GPUs (1 or 2)\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1\n",
    "\n",
    "#SBATCH --time=1:00:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load miniconda3\n",
    "\n",
    "# Include commands in output:\n",
    "set -x\n",
    "\n",
    "# Print current time and date:\n",
    "date\n",
    "\n",
    "# Print host name:\n",
    "hostname\n",
    "\n",
    "# List available GPUs:\n",
    "nvidia-smi\n",
    "\n",
    "# Set environment variables for communication between nodes:\n",
    "export MASTER_PORT=24998\n",
    "export MASTER_ADDR=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)\n",
    "\n",
    "# Run AI scripts:\n",
    "# time conda run -n finetuning --no-capture-output torchrun --nproc_per_node 2 mistral7b_train_ddp.py\n",
    "time srun conda run -n finetuning --no-capture-output torchrun \\\n",
    "    --nnodes=$SLURM_JOB_NUM_NODES \\\n",
    "    --nproc_per_node=$SLURM_GPUS_ON_NODE \\\n",
    "    --rdzv_id=$SLURM_JOB_ID \\\n",
    "    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \\\n",
    "    --rdzv_backend=c10d \\\n",
    "    mistral7b_train_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f174-8231-4e1e-ae39-bff66ffccddc",
   "metadata": {},
   "source": [
    "#### We can now execute the SLURM script and, once the job ran, look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cc6fe-ec18-4856-b99a-e1e2f4f5ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch run_vsc5a100_ddp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886c3e1-da04-49b9-bf9d-806083239ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e52f7e-1937-4f7a-9a26-aff640810487",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -c +0 slurm-3991728.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b3243-4a09-4ebb-8bbc-f1b7edd482f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52fe7c-9bfe-45d0-84ed-9287c9c84f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm mistral7b_train_ddp.py run_vsc5a100_ddp.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf21d02-c9f3-4413-b950-48da652f3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm slurm-*.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b7d0b-924a-46f1-a23f-42ee5964dfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
