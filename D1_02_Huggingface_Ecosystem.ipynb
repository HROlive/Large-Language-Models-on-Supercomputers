{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869f918b-2554-4152-857b-1c38a3e12522",
   "metadata": {},
   "source": [
    "# Huggingface Ecosystem Overview\n",
    "\n",
    "Huggingface is a company that has revolutionized the field of Natural Language Processing (NLP) by providing an open-source library and tools that facilitate the use of state-of-the-art models for various NLP tasks.\n",
    "\n",
    "### Key Components of the Huggingface Ecosystem:\n",
    "\n",
    "1. **Transformers**: A library that provides APIs and tools to easily download and fine-tune state-of-the-art pre-trained models.\n",
    "2. **Datasets**: A library to access and process large datasets used for NLP and other machine learning tasks.\n",
    "3. **PEFT (Parameter-Efficient Fine-Tuning)**: A library for efficient model fine-tuning using parameter-efficient techniques.\n",
    "4. **Accelerate**: A library to accelerate PyTorch and TensorFlow models' training and deployment across multiple devices (GPUs).\n",
    "5. **Huggingface Hub**: A central repository for pre-trained models, datasets, and metrics, allowing seamless sharing and collaboration.\n",
    "\n",
    "Let's explore each component in detail with hands-on examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4c4d1-370f-4c16-a565-b4d5fd8ebce4",
   "metadata": {},
   "source": [
    "## Transformers Library\n",
    "\n",
    "The `transformers` library by Huggingface is a powerful toolkit that provides state-of-the-art pre-trained models and easy-to-use APIs for NLP tasks such as text classification, named entity recognition, translation, text generation, and more.\n",
    "\n",
    "### Key Features:\n",
    "- Provides thousands of pre-trained models.\n",
    "- Supports multiple frameworks: PyTorch, TensorFlow, and JAX.\n",
    "- Easy integration with the Huggingface Hub.\n",
    "\n",
    "### Example: Loading a Pre-trained Model and Running Inference\n",
    "Let's load a pre-trained BERT model and use it for a simple text classification task.\n",
    "For that we use `pipeline()`.\n",
    "`pipeline()` is a very convenient way to use a pretrained model for inference. You can use the `pipeline()` out-of-the-box for many tasks across different modalities, as can be seen [here](https://huggingface.co/docs/transformers/quicktour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa7e310-283a-4f50-abc9-95b48754a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 10:41:34.040792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 10:41:34.040882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 10:41:34.041972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 10:41:34.047420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe69d23a1f84c569702b8be7d6d4663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf27b11f32e4f3393dcd42a7e440146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78657da9ce95413da522892cfc71e143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21b9d6de9b248d3bc3ac68c1ac82ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment-analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8481c1-06ec-43ed-b5fd-7f11de491ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998051524162292}]\n"
     ]
    }
   ],
   "source": [
    "# Test the pipeline with some example text\n",
    "result = classifier(\"Huggingface is an amazing platform for NLP research!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bda56d-26b2-4686-b81b-0ea95eb22aeb",
   "metadata": {},
   "source": [
    "Feel free to try out different sentences, also negative ones.\n",
    "Should you have more than one input, pass them as a list. You will then get a list of dictionaries as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e309aae-5281-4496-a4e0-24a0a04194b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9970724582672119}, {'label': 'NEGATIVE', 'score': 0.99234539270401}]\n"
     ]
    }
   ],
   "source": [
    "result = classifier([\"EuorCC courses are not bad at all. There is lots to gain from them.\",\n",
    "                    \"I think Large Language Models are overrated.\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5b3a0-5abd-439f-bb85-3434364f9299",
   "metadata": {},
   "source": [
    "You can see, that the model not anly predicts the sentiment, but also outputs a score, which is a probability that indicates the model's confidence in its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293fb22-5e39-4db2-a423-668547372757",
   "metadata": {},
   "source": [
    "## Datasets Library\n",
    "\n",
    "The `datasets` library provides a lightweight library for easily downloading and using datasets in NLP and other ML domains. It is optimized for both in-memory and out-of-memory (on-disk) use, making it suitable for handling very large datasets.\n",
    "\n",
    "### Key Features:\n",
    "- Access to thousands of datasets in various domains.\n",
    "- Built-in data processing tools such as caching, shuffling, and batching.\n",
    "- Easy integration with the `transformers` library for model training.\n",
    "\n",
    "### Example: Loading and Exploring a Dataset\n",
    "Let's load a sample dataset and explore its content.\n",
    "The **IMDB** dataset is a popular benchmark dataset used for sentiment analysis tasks in natural language processing. It consists of movie reviews from the Internet Movie Database (IMDB) and is specifically designed for binary sentiment classification: determining whether a given movie review expresses a positive or negative sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3d7a02-bc94-4b72-81d6-a625a49c4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751799cb-ccd6-499d-bf29-45a267224d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn't it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You're supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten's part seemed forced; added. She doesn't have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you're millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it's an irritating snore fest. That's what happens when you're out of touch. You entertain your few friends with inside jokes, and bore all the rest.\", 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Display the 10th example in the training set\n",
    "print(dataset['train'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f351ea-702a-4a65-8780-fe2c3e4b12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ebe225-aefa-40a1-8d7f-f8b84fefc0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a55c90b6eb4bc588914c84a9421f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model for binary classification (num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477be9ec-41fc-4943-8402-bac5088eb361",
   "metadata": {},
   "source": [
    "The Warning we got is to be expected, because the classification head we added has not been pre-trained and the weights and biases have been newly initialized.\n",
    "\n",
    "The bert-base-uncased model is a general-purpose, pre-trained BERT model. It has been trained on a large corpus of text using self-supervised objectives (like masked language modeling) but not for specific tasks like sentiment analysis, classification, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c2956f-8141-4c4b-85b4-4ea896b19a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for efficient fine-tuning\n",
    "config = LoraConfig(r=8)\n",
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65bc53-16a6-4941-8023-f17dcd1d7779",
   "metadata": {},
   "source": [
    "`get_peft_model` is a function from the PEFT library that takes a pre-trained model and a LoRA configuration (`LoraConfig`) and returns a new model that has been adapted for parameter-efficient fine-tuning.\n",
    "The new model (`peft_model`) has the same architecture as the original model (model) but with additional parameters introduced by LoRA that enable efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde9af54-809e-49fc-8450-d217dd133a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7dc50a-3108-4f81-a0b6-a8bf77ecad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d41bfd6-e79a-4bda-8484-db7b0bbb7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5999d9c-39a2-46e5-ad17-b38179d80233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9402f4478142e9954ff8ab3fd81cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d74cc708b444bbcb7b51f2e0e62f773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2cb7932574a468bcec27d1f264a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format to include PyTorch tensors for input_ids, attention_mask, and label\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Rename 'label' column to 'labels'\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2da5a4-7154-4f74-8240-512d1ec18ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(output_dir=\"./results\",\n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  report_to=\"none\",\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb74a845-84df-4547-a8cb-ff049a23d648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/opt/sw/jupyterhub/envs/conda/vsc5/jupyterhub-huggingface-v2/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 03:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=782, training_loss=0.6659453409102262, metrics={'train_runtime': 196.9307, 'train_samples_per_second': 126.948, 'train_steps_per_second': 3.971, 'total_flos': 1650106406400000.0, 'train_loss': 0.6659453409102262, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(model=peft_model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_datasets['train'],\n",
    "                  eval_dataset=tokenized_datasets['test'])\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858ddfb-7266-4dc5-b2d1-2e93e2770c91",
   "metadata": {},
   "source": [
    "## Huggingface Hub\n",
    "\n",
    "The Huggingface Hub is a platform for sharing models, datasets, and demos. It allows developers and researchers to collaborate, publish, and discover models and datasets, making the entire community's work more accessible.\n",
    "\n",
    "### Key Features:\n",
    "- A central repository for models, datasets, and metrics.\n",
    "- Tools for versioning, collaboration, and deployment.\n",
    "- Integrated with Huggingface libraries for easy use.\n",
    "\n",
    "### Example: Uploading a Model to the Huggingface Hub\n",
    "Here's how you can upload a model to the Huggingface Hub.\n",
    "\n",
    "``` python\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# Example of creating a new repository (Requires authentication)\n",
    "repo_name = \"my-awesome-model\"\n",
    "api.create_repo(repo_name)\n",
    "\n",
    "# Save model locally and push to hub\n",
    "model.save_pretrained(f\"./{repo_name}\")\n",
    "api.upload_folder(repo_id=repo_name, folder_path=f\"./{repo_name}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04781ed0-9729-47ab-9b64-5f14a0716bc4",
   "metadata": {},
   "source": [
    "### Explore Available Models\n",
    "\n",
    "We will use the Huggingface API to search for available models and filter them based on certain criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b36aa4a-1f6a-44ca-a1be-7c7d6342bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4203ce2d-9dd6-4a12-bf7e-7628e3d64074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Huggingface API\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5beccaf-410f-4c0e-8728-5fae880b2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for models in the Huggingface Model Hub\n",
    "models = list(api.list_models(limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fd92d8-796e-4eaf-b226-4831a4732c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert/albert-base-v1\n",
      "albert/albert-base-v2\n",
      "albert/albert-large-v1\n",
      "albert/albert-large-v2\n",
      "albert/albert-xlarge-v1\n",
      "albert/albert-xlarge-v2\n",
      "albert/albert-xxlarge-v1\n",
      "albert/albert-xxlarge-v2\n",
      "google-bert/bert-base-cased-finetuned-mrpc\n",
      "google-bert/bert-base-cased\n"
     ]
    }
   ],
   "source": [
    "# Display the fetched models\n",
    "for model in models:\n",
    "    print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0111f01b-8a5d-4059-9d74-f275b2c16a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(id='albert/albert-base-v1', author=None, sha=None, created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), last_modified=None, private=False, gated=None, disabled=None, downloads=15530, likes=8, library_name='transformers', tags=['transformers', 'pytorch', 'tf', 'safetensors', 'albert', 'fill-mask', 'exbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, siblings=None, spaces=None, safetensors=None),\n",
       " ModelInfo(id='albert/albert-base-v2', author=None, sha=None, created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), last_modified=None, private=False, gated=None, disabled=None, downloads=1890109, likes=104, library_name='transformers', tags=['transformers', 'pytorch', 'tf', 'jax', 'rust', 'safetensors', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, siblings=None, spaces=None, safetensors=None),\n",
       " ModelInfo(id='albert/albert-large-v1', author=None, sha=None, created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), last_modified=None, private=False, gated=None, disabled=None, downloads=1522, likes=3, library_name='transformers', tags=['transformers', 'pytorch', 'tf', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, siblings=None, spaces=None, safetensors=None),\n",
       " ModelInfo(id='albert/albert-large-v2', author=None, sha=None, created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), last_modified=None, private=False, gated=None, disabled=None, downloads=11271, likes=16, library_name='transformers', tags=['transformers', 'pytorch', 'tf', 'safetensors', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, siblings=None, spaces=None, safetensors=None),\n",
       " ModelInfo(id='albert/albert-xlarge-v1', author=None, sha=None, created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), last_modified=None, private=False, gated=None, disabled=None, downloads=798, likes=4, library_name='transformers', tags=['transformers', 'pytorch', 'tf', 'safetensors', 'albert', 'fill-mask', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1909.11942', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, siblings=None, spaces=None, safetensors=None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 models for demonstration\n",
    "models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b06c4-f6ed-4190-978c-c7de60c33b1d",
   "metadata": {},
   "source": [
    "Should you with to make that visually more pleasing, you can do so, by creating a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373fd47d-fac4-4ea6-bf6c-ddbb99b490ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "070ed909-632a-40b1-aa46-0d21233aa3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame by first creating a dictionary:\n",
    "model_data = []\n",
    "\n",
    "for model in models:\n",
    "    model_info = {\n",
    "        'Model ID': model.modelId,\n",
    "        'Tags': ', '.join(model.tags) if model.tags else 'N/A',\n",
    "        'Downloads': model.downloads,\n",
    "        'Likes': model.likes,\n",
    "        'Pipeline Tag': model.pipeline_tag if model.pipeline_tag else 'N/A',\n",
    "        'Last Modified': model.lastModified.strftime('%Y-%m-%d') if model.lastModified else 'N/A'\n",
    "    }\n",
    "    model_data.append(model_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a88db43-e578-4897-9798-801612f8d839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Downloads</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Pipeline Tag</th>\n",
       "      <th>Last Modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert/albert-base-v1</td>\n",
       "      <td>transformers, pytorch, tf, safetensors, albert...</td>\n",
       "      <td>15530</td>\n",
       "      <td>8</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert/albert-base-v2</td>\n",
       "      <td>transformers, pytorch, tf, jax, rust, safetens...</td>\n",
       "      <td>1890109</td>\n",
       "      <td>104</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert/albert-large-v1</td>\n",
       "      <td>transformers, pytorch, tf, albert, fill-mask, ...</td>\n",
       "      <td>1522</td>\n",
       "      <td>3</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert/albert-large-v2</td>\n",
       "      <td>transformers, pytorch, tf, safetensors, albert...</td>\n",
       "      <td>11271</td>\n",
       "      <td>16</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert/albert-xlarge-v1</td>\n",
       "      <td>transformers, pytorch, tf, safetensors, albert...</td>\n",
       "      <td>798</td>\n",
       "      <td>4</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model ID                                               Tags  \\\n",
       "0    albert/albert-base-v1  transformers, pytorch, tf, safetensors, albert...   \n",
       "1    albert/albert-base-v2  transformers, pytorch, tf, jax, rust, safetens...   \n",
       "2   albert/albert-large-v1  transformers, pytorch, tf, albert, fill-mask, ...   \n",
       "3   albert/albert-large-v2  transformers, pytorch, tf, safetensors, albert...   \n",
       "4  albert/albert-xlarge-v1  transformers, pytorch, tf, safetensors, albert...   \n",
       "\n",
       "   Downloads  Likes Pipeline Tag Last Modified  \n",
       "0      15530      8    fill-mask           N/A  \n",
       "1    1890109    104    fill-mask           N/A  \n",
       "2       1522      3    fill-mask           N/A  \n",
       "3      11271     16    fill-mask           N/A  \n",
       "4        798      4    fill-mask           N/A  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the dictionary to pandas DataFrame:\n",
    "df_models = pd.DataFrame(model_data)\n",
    "\n",
    "# Display the first 5 entries of the DataFrame:\n",
    "df_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54fa57-1f74-4de2-ae98-c914f7413874",
   "metadata": {},
   "source": [
    "#### Analyze Model Information\n",
    "\n",
    "Inspect the models to understand their details, such as architecture, number of parameters, and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c2e6aeb-6863-4c8a-9d11-40180f8d58c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google-bert/bert-base-uncased\n",
      "Description: No description available\n",
      "Framework: fill-mask\n",
      "Tags: ['transformers', 'pytorch', 'tf', 'jax', 'rust', 'coreml', 'onnx', 'safetensors', 'bert', 'fill-mask', 'exbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1810.04805', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n"
     ]
    }
   ],
   "source": [
    "# Display information about a specific model\n",
    "model_name = \"bert-base-uncased\"  # Example model\n",
    "model_info = api.model_info(model_name)\n",
    "\n",
    "print(f\"Model: {model_info.modelId}\")\n",
    "print(f\"Description: {model_info.cardData.get('description', 'No description available')}\")\n",
    "print(f\"Framework: {model_info.pipeline_tag}\")\n",
    "print(f\"Tags: {model_info.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9df14e-6292-4df7-afd7-89c2cab531aa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the Huggingface ecosystem, including the `transformers`, `datasets`, and Huggingface Hub.\n",
    "We will get to know `PEFT` and `accelerate` in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2518fdea-817b-487e-b882-c6bc942c2eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd46d3a-2ab5-4771-a979-6980e17de4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
