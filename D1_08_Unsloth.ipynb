{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c272574-6a76-40c2-b7fd-9cadce7ef1c5",
   "metadata": {},
   "source": [
    "## Unsloth: Optimizing Training and Inference Performance\n",
    "\n",
    "For many software algorithms, the performance does not only depend on the number and kind of calculations performed. Instead, the exact order and the size of chunks has an enormous influence on the calculation speed.\n",
    "For large language models, a library called `unsloth` contains optimized GPU kernels created by manually deriving all compute heavy math steps. By using these optimized kernels, a significant speed-up can be obtained.\n",
    "\n",
    "### Key Techniques in Unsloth:\n",
    "\n",
    "1. **Efficient Data Loading**: Optimizing data pipelines to reduce latency and improve throughput during training.\n",
    "2. **Batching and Padding Strategies**: Dynamically adjusting batch sizes and minimizing padding to optimize memory usage.\n",
    "3. **Half-Precision and Quantized Inference**: Using mixed precision or quantized models to speed up inference and reduce memory footprint.\n",
    "4. **Model Pruning and Distillation**: Reducing the size of the model by removing redundant parameters or training smaller models to mimic larger ones.\n",
    "\n",
    "### Benefits of Unsloth:\n",
    "\n",
    "- **Reduced Training Time**: Optimizing data loading and model architecture reduces the time required for each epoch.\n",
    "- **Lower Memory Usage**: Using techniques like mixed precision and quantization reduces the amount of GPU memory required.\n",
    "- **Faster Inference**: Optimizing the model for deployment can significantly reduce latency during inference.\n",
    "\n",
    "### Hands-On Example: Efficient Data Loading and Mixed Precision Training\n",
    "\n",
    "In this example, we take the example from the previous notebook (\"PEFT\") and adjust them to use `unsloth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6628f19-3f7b-463e-8c74-928f676eb61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unsloth_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unsloth_demo.py\n",
    "# Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from textwrap import dedent  # Remove leading whitespace from multiline strings\n",
    "## Instead of:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "## use:\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "## Instead of:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(...)\n",
    "## use: \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    ## Instead of:\n",
    "    # quantization_config=BitsAndBytesConfig(...)\n",
    "    ## use:\n",
    "    load_in_4bit=True,\n",
    "    # device_map='cuda:0',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')\n",
    "\n",
    "## Instead of:\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type='CAUSAL_LM',\n",
    "#     r=16,\n",
    "#     lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "#     bias='none',\n",
    "#     target_modules='all-linear',\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "## use:\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0,  # Unsloth supports any, but = 0 is optimized\n",
    "    bias='none',  # Unsloth supports any, but = 'none' is optimized\n",
    "    # Unsloth does not allow 'all-linear' => manually specify target modules: \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing='unsloth',  # True or 'unsloth' for very long context\n",
    ")\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    # When using newer versions of `trl`, use SFTConfig(...) instead of TrainingArguments(...).\n",
    "    output_dir='phi-3-mini-instruct-guanaco',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 12 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 8 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=20,\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training: faster, but uses more memory\n",
    "    report_to='none',  # disable wandb\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    eval_dataset=guanaco_test,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation on test dataset before finetuning:\")\n",
    "print(eval_result)\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training result:\")\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation on test dataset after finetuning:\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8502391-5522-40ad-95f9-28fda50b9540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unsloth_demo.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile unsloth_demo.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=zen3_0512_a100x2\n",
    "# #SBATCH --qos=zen3_0512_a100x2\n",
    "#SBATCH --qos=admin\n",
    "#SBATCH --gres=gpu:1  # Number of GPUs (1 or 2)\n",
    "#SBATCH --time=0:20:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load miniconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "conda run -n finetuning --no-capture-output python unsloth_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd37d15b-90d4-4866-9aaf-684aabf0cc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch: Allocating 50.0 % of cpu resources: 64 / 128.\n",
      "sbatch: Number of tasks adjusted to 64.\n",
      "Submitted batch job 3982157\n"
     ]
    }
   ],
   "source": [
    "!sbatch unsloth_demo.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "132e85b6-dd54-4bee-9538-78f67732209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID            PARTITION     NAME     USER ST       TIME  NODES     NODELIST(REASON)\n",
      "           3982157     zen3_0512_a100x2 unsloth_ mpfister  R       0:24      1            n3072-015\n",
      "           3979795     zen3_0512_a100x2 vsc5_jh_ mpfister  R   10:52:24      1            n3071-001\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337490a4-0117-41eb-8678-dde3c7e5a33e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9657ac-ed45-49e4-a567-4628e4c8483f",
   "metadata": {},
   "source": [
    "```\n",
    "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
    "Unsloth: WARNING `trust_remote_code` is True.\n",
    "Are you certain you want to do remote code execution?\n",
    "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.43.4.\n",
    "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.393 GB. Platform = Linux.\n",
    "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = True]\n",
    " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9846/9846 [00:01<00:00, 7331.67 examples/s]\n",
    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 7213.22 examples/s]\n",
    "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
    "max_steps is given, it will override any value given in num_train_epochs\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:26<00:00,  2.45it/s]\n",
    "Evaluation on test dataset before finetuning:\n",
    "{'eval_loss': 1.4085867404937744, 'eval_model_preparation_time': 0.006, 'eval_runtime': 61.1316, 'eval_samples_per_second': 8.474, 'eval_steps_per_second': 1.063}\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
    "   \\\\   /|    Num examples = 9,846 | Num Epochs = 1\n",
    "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
    "\\        /    Total batch size = 8 | Total steps = 100\n",
    " \"-____-\"     Number of trainable parameters = 29,884,416\n",
    "{'loss': 1.2382, 'grad_norm': 0.41076236963272095, 'learning_rate': 0.00018, 'epoch': 0.01}\n",
    "{'loss': 1.1579, 'grad_norm': 0.3123883306980133, 'learning_rate': 0.00016, 'epoch': 0.02}\n",
    "{'loss': 1.1584, 'grad_norm': 0.33330079913139343, 'learning_rate': 0.00014, 'epoch': 0.02}\n",
    "{'loss': 1.1558, 'grad_norm': 0.2736963927745819, 'learning_rate': 0.00012, 'epoch': 0.03}\n",
    "{'loss': 1.1786, 'grad_norm': 0.30604463815689087, 'learning_rate': 0.0001, 'epoch': 0.04}\n",
    "{'loss': 1.2531, 'grad_norm': 0.21231390535831451, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
    "{'loss': 1.2122, 'grad_norm': 0.21684657037258148, 'learning_rate': 6e-05, 'epoch': 0.06}\n",
    "{'loss': 1.1345, 'grad_norm': 0.17966635525226593, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
    "{'loss': 1.1084, 'grad_norm': 0.2505042850971222, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
    "{'loss': 1.1221, 'grad_norm': 0.21714507043361664, 'learning_rate': 0.0, 'epoch': 0.08}\n",
    "{'train_runtime': 169.8121, 'train_samples_per_second': 4.711, 'train_steps_per_second': 0.589, 'train_loss': 1.1719264698028564, 'epoch': 0.08}\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:49<00:00,  1.70s/it]\n",
    "Training result:\n",
    "TrainOutput(global_step=100, training_loss=1.1719264698028564, metrics={'train_runtime': 169.8121, 'train_samples_per_second': 4.711, 'train_steps_per_second': 0.589, 'total_flos': 1.551664613154816e+16, 'train_loss': 1.1719264698028564, 'epoch': 0.08123476848090982})\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:27<00:00,  2.38it/s]\n",
    "Evaluation on test dataset after finetuning:\n",
    "{'eval_loss': 1.2287541627883911, 'eval_model_preparation_time': 0.006, 'eval_runtime': 27.6746, 'eval_samples_per_second': 18.718, 'eval_steps_per_second': 2.349, 'epoch': 0.08123476848090982}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322095e-e088-4a3f-a66f-7e8dddd46458",
   "metadata": {},
   "source": [
    "### Just for comparison, the same code without unsloth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6090d0d-c85a-4ccb-a3b2-67b12f38266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unsloth_demo_nounsloth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unsloth_demo_nounsloth.py\n",
    "# Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from textwrap import dedent  # Remove leading whitespace from multiline strings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16),\n",
    "    device_map='cuda:0',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')\n",
    "guanaco_test = load_dataset('timdettmers/openassistant-guanaco', split='test')\n",
    "\n",
    "## Instead of:\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=16,\n",
    "    lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "    bias='none',\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    # When using newer versions of `trl`, use SFTConfig(...) instead of TrainingArguments(...).\n",
    "    output_dir='phi-3-mini-instruct-guanaco',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 12 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 8 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=20,\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training: faster, but uses more memory\n",
    "    report_to='none',  # disable wandb\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    eval_dataset=guanaco_test,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation on test dataset before finetuning:\")\n",
    "print(eval_result)\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training result:\")\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation on test dataset after finetuning:\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdc04584-0b52-403f-8fc6-2dae3fe8bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unsloth_demo_nounsloth.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile unsloth_demo_nounsloth.slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=zen3_0512_a100x2\n",
    "# #SBATCH --qos=zen3_0512_a100x2\n",
    "#SBATCH --qos=admin\n",
    "#SBATCH --gres=gpu:1  # Number of GPUs (1 or 2)\n",
    "#SBATCH --time=0:20:00\n",
    "\n",
    "# Load conda:\n",
    "module purge\n",
    "module load miniconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "conda run -n finetuning --no-capture-output python unsloth_demo_nounsloth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "046096b1-2792-4aaf-a128-e022177486e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch: Allocating 50.0 % of cpu resources: 64 / 128.\n",
      "sbatch: Number of tasks adjusted to 64.\n",
      "Submitted batch job 3982161\n"
     ]
    }
   ],
   "source": [
    "!sbatch unsloth_demo_nounsloth.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fec2037b-a43c-485d-89c8-0876133451c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID            PARTITION     NAME     USER ST       TIME  NODES     NODELIST(REASON)\n",
      "           3982161     zen3_0512_a100x2 unsloth_ mpfister  R       0:25      1            n3072-015\n",
      "           3982160     zen3_0512_a100x2 unsloth_ mpfister  R       3:28      1            n3072-015\n",
      "           3979795     zen3_0512_a100x2 vsc5_jh_ mpfister  R   10:59:48      1            n3071-001\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0339e-45c2-4aae-afd8-a07a9735a59e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5635ade-e2de-47eb-a6b2-4d21c5541d7d",
   "metadata": {},
   "source": [
    "```\n",
    "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.48s/it]\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9846/9846 [00:01<00:00, 7429.40 examples/s]\n",
    "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 7066.79 examples/s]\n",
    "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
    "max_steps is given, it will override any value given in num_train_epochs\n",
    "You are not running the flash-attention implementation, expect numerical differences.\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:44<00:00,  1.47it/s]\n",
    "Evaluation on test dataset before finetuning:\n",
    "{'eval_loss': 1.4072628021240234, 'eval_model_preparation_time': 0.0026, 'eval_runtime': 46.7094, 'eval_samples_per_second': 11.09, 'eval_steps_per_second': 1.392}\n",
    "  0%|          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
    "{'loss': 1.2626, 'grad_norm': 0.2466505616903305, 'learning_rate': 0.00018, 'epoch': 0.01}\n",
    "{'loss': 1.1671, 'grad_norm': 0.22209370136260986, 'learning_rate': 0.00016, 'epoch': 0.02}\n",
    "{'loss': 1.1649, 'grad_norm': 0.22614338994026184, 'learning_rate': 0.00014, 'epoch': 0.02}\n",
    "{'loss': 1.1588, 'grad_norm': 0.19410459697246552, 'learning_rate': 0.00012, 'epoch': 0.03}\n",
    "{'loss': 1.1821, 'grad_norm': 0.22094522416591644, 'learning_rate': 0.0001, 'epoch': 0.04}\n",
    "{'loss': 1.257, 'grad_norm': 0.16384980082511902, 'learning_rate': 8e-05, 'epoch': 0.05}\n",
    "{'loss': 1.2183, 'grad_norm': 0.1637776792049408, 'learning_rate': 6e-05, 'epoch': 0.06}\n",
    "{'loss': 1.1377, 'grad_norm': 0.14242839813232422, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
    "{'loss': 1.1138, 'grad_norm': 0.19825458526611328, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
    "{'loss': 1.1256, 'grad_norm': 0.16682812571525574, 'learning_rate': 0.0, 'epoch': 0.08}\n",
    "{'train_runtime': 237.3404, 'train_samples_per_second': 3.371, 'train_steps_per_second': 0.421, 'train_loss': 1.1787943458557129, 'epoch': 0.08}\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:57<00:00,  2.37s/it]\n",
    "Training result:\n",
    "TrainOutput(global_step=100, training_loss=1.1787943458557129, metrics={'train_runtime': 237.3404, 'train_samples_per_second': 3.371, 'train_steps_per_second': 0.421, 'total_flos': 1.542992772194304e+16, 'train_loss': 1.1787943458557129, 'epoch': 0.08123476848090982})\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:45<00:00,  1.43it/s]\n",
    "Evaluation on test dataset after finetuning:\n",
    "{'eval_loss': 1.2330236434936523, 'eval_model_preparation_time': 0.0026, 'eval_runtime': 46.069, 'eval_samples_per_second': 11.244, 'eval_steps_per_second': 1.411, 'epoch': 0.08123476848090982}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
