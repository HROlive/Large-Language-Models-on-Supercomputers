{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8807f6-05c9-4b14-bbf1-9eb0750c0bc7",
   "metadata": {},
   "source": [
    "## Huggingface Accelerate\n",
    "\n",
    "In this notebook, we are going to write a Python and SLURM script directly from within cells here and also launch the SLURM script.\n",
    "Since we only have limited ressources, we are going to use a small dateset in combination with a small model. However, this still demonstrates how to use Huggingface's [Accelerate](https://huggingface.co/docs/accelerate/index) library to perform distributed training on multiple GPUs across multiple nodes. In our case, we are using 2 nodes with 2 NVIDIA A100 GPUs.\n",
    "The network used on VSC5 is Infiniband.\n",
    "\n",
    "### Python script\n",
    "Let's go through the Python script, so you know what we are launching here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9b2787-0fa5-449a-a83c-c4a3fcf2a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./examples/accelerate_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./examples/accelerate_example.py\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"ag_news\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"train\"], batch_size=8, shuffle=True)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"test\"], batch_size=8)\n",
    "\n",
    "    # Prepare optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Prepare everything using accelerator\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3ad00-b24d-496e-b65a-80c322f9bb54",
   "metadata": {},
   "source": [
    "### SLURM Script\n",
    "Now, let's go through the SLURM script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e298998-668e-471e-aea3-f2a6890f3627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tooling/slurm_scripts/accelerate_example.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tooling/slurm_scripts/accelerate_example.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=training_example\n",
    "#SBATCH --account=p71550\n",
    "##SBATCH --account=p70824 # training account, please uncomment for training\n",
    "#SBATCH --nodes=2                    # Number of nodes\n",
    "#SBATCH --ntasks-per-node=1          # Number of tasks per node\n",
    "#SBATCH --cpus-per-task=256          # Number of CPU cores per task (including hyperthreading if needed)\n",
    "#SBATCH --partition=zen3_0512_a100x2\n",
    "#SBATCH --qos=admin\n",
    "##SBATCH --qos=zen3_0512_a100x2 # qos for training\n",
    "#SBATCH --gres=gpu:2                 # Number of GPUs per node\n",
    "#SBATCH --output=../../output/%x-%j.out  # Output file\n",
    "##SBATCH --reservation=\n",
    "\n",
    "######################\n",
    "### Set Environment ###\n",
    "######################\n",
    "module load miniconda3\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "source /opt/sw/jupyterhub/envs/conda/vsc5/jupyterhub-huggingface-v2/modules  # Activate the conda environment\n",
    "\n",
    "######################\n",
    "#### Set Network #####\n",
    "######################\n",
    "# Get the IP address of the master node (head node)\n",
    "nodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\")\n",
    "nodes_array=($nodes)\n",
    "node_0=${nodes_array[0]}\n",
    "\n",
    "NUM_PROCESSES=$(( SLURM_NNODES * SLURM_GPUS_ON_NODE ))\n",
    "\n",
    "export MASTER_ADDR=$node_0\n",
    "export MASTER_PORT=29500\n",
    "\n",
    "######################\n",
    "#### Prepare Launch ###\n",
    "######################\n",
    "# Configure Accelerate launch command\n",
    "\n",
    "export LAUNCHER=\"accelerate launch \\\n",
    "    --config_file \"../config/accelerate_default_config.yaml\" \\\n",
    "    --machine_rank \\$SLURM_PROCID \\\n",
    "    --main_process_ip $MASTER_ADDR \\\n",
    "    --main_process_port $MASTER_PORT \\\n",
    "    --num_processes $NUM_PROCESSES \\\n",
    "    --num_machines $SLURM_NNODES \\\n",
    "    \"\n",
    "export PROGRAM=\"../../examples/accelerate_example.py\"\n",
    "\n",
    "START=$(date +%s.%N)\n",
    "echo \"START TIME: $(date)\"\n",
    "\n",
    "export SRUN_ARGS=\"--cpus-per-task $SLURM_CPUS_PER_TASK --jobid $SLURM_JOBID\"\n",
    "export OMP_NUM_THREADS=256\n",
    "export CMD=\"$LAUNCHER $PROGRAM\"\n",
    "\n",
    "# Execute the command with srun to run on multiple nodes\n",
    "srun $SRUN_ARGS ../start_train.sh \"$CMD\"\n",
    "\n",
    "echo \"END TIME: $(date)\"\n",
    "END=$(date +%s.%N)\n",
    "RUNTIME=$(echo \"$END - $START\" | bc -l)\n",
    "echo \"Runtime: $RUNTIME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc2b6b-f405-4da8-a97b-d27ac4a6345c",
   "metadata": {},
   "source": [
    "### Unload Env Variables\n",
    "Before we can launch this SLURM script from within our Jupyter notebook, we need to source a script, which esures that we unload all Jupyterhub related environment variables, and functions, etc. You do not need this from outside the Jupyterhub!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d48db6-a0f2-4ffd-96ca-6e84bc95ba7a",
   "metadata": {},
   "source": [
    "`!source ./tooling/unload_jupyter_env.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7e349-3960-4ca8-9edd-7367d96698fd",
   "metadata": {},
   "source": [
    "### Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55204d-1b0b-414b-a299-32863b8f49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ./tooling/unload_jupyter_env.sh && sbatch ./tooling/slurm_scripts/accelerate_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13647c18-2068-4f0e-868b-1c7e0c38d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab8384-4bc1-43f0-9034-26ee4ee8b81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb6f59-e8b9-41f6-bdd1-a9e8ce5005ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
